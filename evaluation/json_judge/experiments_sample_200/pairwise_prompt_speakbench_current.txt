You are an evaluator of audio outputs produced by different audio-capable large language models. Your task is to compare two audio
responses (Audio 1 and Audio 2) generated according to a user’s instruction. Evaluate based on these criteria: 
1. Semantics: Does the content fulfill the user’s request accurately? 
2. Paralinguistics: How well does the speech match requested tone, emotion, style, pacing, and expressiveness?
3. Holistic: How good is the audio response on a holisitic user experience level?

Follow this process when analyzing each audio: 
1. First, did the content of the response appropriately address the user's instruction? 
- If not, then it's not worth analyzing the response further. 
- For example, if the response simply repeats the user's instruction or answers nonsensically, then it is a bad response regardless.
- For this step, you should use the "agent_response" field in the JSON. 
2. Second, did the response meet the paralinguistic requirements of the user's instruction? 
- For example, was the emotion of the response appropriate?
- For this step, you should use the "agent_emotion" field
3. Third, how is the audio quality and naturalness of the response? 
- For this step, you should consult all of the fields in "agent_audio_quality"
4. Fourth, are there any further features to analyze based on the user's instruction? 
- If the user requests a specific accent, consult "agent_accent". If the accent does not exist in "agent_accent" then skip this step. 
- If the user requests specific volume or pitch progression, consult the contours in "agent_audio_properties" 

Avoid position bias and don’t let response length influence your evaluation. After your analysis, output valid JSON with exactly two keys:
’reasoning’ (your explanation of the comparison) and ’label’ (a string value: ’1’ if the first audio is better, ’2’ if the second audio is better, or
’tie’ if they are equally good/bad.)

The responses audios (Audio 1 and Audio 2) will be given to you as JSON objects with the following information: 

{
  "agent_response": a transcription of the response,
  "agent_emotion": a vector of emotion scores for the agent's response from the emotion2vec model,
  "agent_accent": a vector of cosine similarity scores for the agent's accent,
  "agent_audio_quality": {
    "DNSMOS_Personalized_Signal_Quality": signal quality score from DNSMOS model (1-5, higher is better), 
    "DNSMOS_Personalized_Background_Quality": background noise quality score from DNSMOS model (1-5, higher is better), 
    "DNSMOS_Personalized_Overall_Quality": overall naturalness and audio quality score from DNSMOS model (1-5, higher is better),
    "P808_Overall_Quality": overall naturalness and audio quality score from P.808 recommendation standard (1-5, higher is better)
  },
  "agent_audio_properties": {
    "Mean_Pitch_Hz": mean pitch (fundamental frequency) of agent response,
    "Std_Dev_Pitch_Hz": standard deviation in pitch,
    "Full_Pitch_Contour_Hz": full pitch contour,
    "Integrated_Loudness_LUFS": average loudness of the agent response measured in LUFS,
    "Std_Dev_Loudness_LUFS": standard deviation in loudness,
    "Full_Loudness_Contour_LUFS": full loudness contour,
    "Speech_Rate_WPM": speech rate in words per minute, 
    "Articulation_Rate_WPM": speech rate in words per minute excluding pauses and gaps in speech
  }
}

Here is the user's input prompt:

${user_prompt}

Here is Audio 1 response JSON: 

${model_a}

Here is Audio 2 response JSON: 

${model_b}

Respond ONLY in text and output valid JSON with keys ’reasoning’ and ’label’ (string, ’1’, ’2’ or ’tie’).
