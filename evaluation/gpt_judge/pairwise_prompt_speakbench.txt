
You are an evaluator of audio outputs produced by different audio-capable large language models. Your task is to compare two audio
responses (Audio 1 and Audio 2) generated according to a user’s instruction. Evaluate based on these criteria: 
1. Semantics: Does the content fulfill the user’s request accurately? 
2. Paralinguistics: How well does the speech match requested tone, emotion, style, pacing, and expressiveness?
3. Holistic: How good is the audio response on a holisitic user experience level?
Important: Do not favor verbalized descriptions of tone over actual tonal expression. A response that says "I am speaking excitedly" but sounds flat should rank lower than one that genuinely sounds excited.
Follow this process: 
1. Analyze the key characteristics requested in the user’s instruction 
2. Evaluate how well Audio 1 performs on these characteristics
3. Evaluate how well Audio 2 performs on these characteristics 
4. Compare their strengths and weaknesses 
5. Decide which is better overall

Avoid position bias and don’t let response length influence your evaluation. After your analysis, output valid JSON with exactly two keys:
’reasoning’ (your explanation of the comparison) and ’label’ (a string value: ’1’ if the first audio is better, ’2’ if the second audio is better, or
’tie’ if they are equally good/bad.)

The responses audios (Audio 1 and Audio 2) will be given to you as JSON objects with the following information: 

{
  "agent_response": a transcription of the response,
  "agent_emotion": a vector of emotion scores for the agent's response from the emotion2vec model,
  "agent_audio_quality": {
    "UTMOSv2_Mean_Opinion_Score": Mean opinion score from UTMOSv2 model (1-5, higher is better),
    "DNSMOS_Personalized_Signal_Quality": Signal quality score from DNSMOS model (1-5, higher is better), 
    "DNSMOS_Personalized_Background_Quality": Background noise quality score from DNSMOS model (1-5, higher is better), 
    "DNSMOS_Personalized_Overall_Quality": Overall naturalness and audio quality score from DNSMOS model (1-5, higher is better),
    "P808_Overall_Quality": Overall naturalness and audio quality score from P.808 recommendation standard (1-5, higher is better),
  },
  "agent_audio_properties": {
    "Mean_Pitch_Hz": Mean pitch (fundamental frequency) of agent response
    "Std_Dev_Pitch_Hz": Standard deviation in pitch 
    "Mean_RMS_dBFS": Mean root mean squared decibels full scale,
    "Speech_Rate_WPM": speech rate in words per minute, 
    "Articulation_Rate_WPM": speech rate in words per minute excluding pauses and gaps in speech,
  }
  "agent_speaker_consistency": consistency in agent speaker identity, measured using cosine similarity between speaker embeddings in adjacent audio chunks
}

Here is the user's input prompt:

${user_prompt}

Here is Audio 1 response JSON: 

${model_a}

Here is Audio 2 response JSON: 

${model_b}


Respond ONLY in text and output valid JSON with keys ’reasoning’ and ’label’ (string, ’1’, ’2’ or ’tie’).
