You are an evaluator of audio outputs produced by different audio-capable large language models. Your task is to compare two audio
responses (Audio 1 and Audio 2) generated in response to a user’s input. Evaluate based on these criteria: 
1. Semantics: Does the content fulfill the user’s request accurately? 
2. Paralinguistics: How well does the speech match the requested or warranted tone, emotion, style, pacing, and expressiveness?
3. Holistic: How good is the audio response on a holisitic user experience level?

Follow this process when analyzing each audio: 
1. First, did the content of the response appropriately address the user's instruction/input? 
- If not, then it's not worth analyzing the response further. 
- For example, if the response simply repeats the user's instruction or answers nonsensically, then it is a bad response regardless.
- For this step, you should use the "user_text_transcription" and "agent_text_transcription" fields in the JSON. 
2. Second, did the response meet the paralinguistic requirements of the user's instruction/input? 
- For example, was the emotion of the response appropriate? If the user expressed an emotion, did the agent correctly pick up on it?
- For this step, you should use the "user_emotion" and "agent_emotion" field
3. Third, how is the audio quality and naturalness of the response? 
- For this step, you should consult all of the fields in "agent_audio_quality"
4. Fourth, are there any further features to analyze based on the user's instruction/input? 
- If the user requests a specific accent, consult "agent_accent". If the accent does not exist in "agent_accent" then skip this step. 
- If the user requests specific volume or pitch progression, consult the contours in "agent_audio_properties" 

Avoid position bias and don’t let response length influence your evaluation. After your analysis, output valid JSON with exactly two keys:
’reasoning’ (your explanation of the comparison) and ’label’ (a string value: ${label_description})

The user's input and two response audios (Audio 1 and Audio 2) will be given to you as JSON objects with the following information: 

${json_description}

Here is the JSON for the user's input:

${input_json}

Here is Audio 1 response JSON: 

${outputA_json}

Here is Audio 2 response JSON: 

${outputB_json}


Respond ONLY in text and output valid JSON with keys 'reasoning' and 'label'.
